import { ConnectorPhase } from '../../shared/enums/connector-phase';
import { HandlerInput } from '../../shared/types/handler-input';
import { TempDataManager } from '../modules/temp-data-manager';
import ExtractorContext from './extractor-context';
import { NetworkAdapter } from '../modules/network-adapter';
declare class PipelineManager {
    private extractorContext;
    private tempDataManager;
    private networkAdapter;
    constructor(extractorContext: ExtractorContext, tempDataManager: TempDataManager, networkAdapter: NetworkAdapter);
    /** pipelines by name (tableName is useful), for initial pipelines table name is undefined. */
    private pipelinesMap;
    private namedPipelineStatus;
    private pipelineDoneEmitter;
    addInitialPipelines(handlerInputs: readonly HandlerInput[]): void;
    addNamedPipeline(name: string, handlerInput: HandlerInput): void;
    runPhaseOnInitialPipelines(phase: ConnectorPhase): Promise<void>;
    runPhaseOnNamedPipeline(phase: ConnectorPhase, name: string): Promise<void>;
    /**
     * Update Extractor Cache for File Based Handler Input/s
     * @param handlerInputs
     */
    processFileBasedParser(handlerInputs: readonly HandlerInput[]): Promise<void>;
    private waitForPipelineToComplete;
    /**
     * Run a whole pipeline cycle for particular table with the given HandlerInput.
     * This is used by the deferred pipeline flow when a DataTable has a deferred HandlerInput.
     *
     * The API will register the table name and handlerInput to pipeline manager before
     * running the pipeline, so the caller does NOT need to call addNamedPipeline beforehand.
     * .
     */
    runNamedPipeline(name: string, handlerInput: HandlerInput): Promise<void>;
    isNamedPipelineComplete(name: string): boolean;
    private addPipelines;
    private runPhase;
}
export default PipelineManager;
